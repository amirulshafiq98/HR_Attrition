{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d754f4b",
   "metadata": {},
   "source": [
    "# HR Attrition Prediction Project: Full Pipeline\n",
    "\n",
    "This notebook details the end-to-end process of building and evaluating machine learning models to predict employee attrition. It covers data loading, comprehensive preprocessing and feature engineering, model training, evaluation, and final data export for Tableau visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup & Data Loading\n",
    "\n",
    "This section sets up the necessary libraries and loads the pre-processed dataset.\n",
    "\n",
    "### 1.1 Import Libraries\n",
    "\n",
    "We begin by importing all necessary Python libraries for data manipulation, machine learning, visualization, and utility functions. Consolidating imports at the top ensures all dependencies are clear and available throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1235916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import warnings # To manage warnings if needed\n",
    "\n",
    "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output during development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c45ab5",
   "metadata": {},
   "source": [
    "### 1.2 Load Cleaned Data\n",
    "\n",
    "The `full summary.csv` dataset, which contains employee `snapshot data`, is loaded into a Pandas DataFrame. The snapshot_date column is converted to datetime objects to enable time-based calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c4df27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id snapshot_date  age        department business_unit  \\\n",
      "0            1    2021-04-30   35  Customer Success    Commercial   \n",
      "1            1    2021-05-31   35  Customer Success    Commercial   \n",
      "2            1    2021-06-30   35  Customer Success    Commercial   \n",
      "3            1    2021-07-31   35  Customer Success    Commercial   \n",
      "4            1    2021-08-31   35  Customer Success    Commercial   \n",
      "\n",
      "        job_title     location  base_salary  bonus_eligible  bonus_pct  ...  \\\n",
      "0  Senior Analyst  Seattle, WA        77073               0        0.0  ...   \n",
      "1  Senior Analyst  Seattle, WA        77073               0        0.0  ...   \n",
      "2  Senior Analyst  Seattle, WA        77073               0        0.0  ...   \n",
      "3  Senior Analyst  Seattle, WA        77073               0        0.0  ...   \n",
      "4  Senior Analyst  Seattle, WA        77073               0        0.0  ...   \n",
      "\n",
      "   job_level  last_training_date vacation_leave sick_leave personal_leave  \\\n",
      "0          1           30/3/2021              0          0              0   \n",
      "1          1           30/3/2021              0          0              0   \n",
      "2          1           30/3/2021              0          0              0   \n",
      "3          1           30/3/2021              0          0              0   \n",
      "4          1           30/3/2021              0          0              0   \n",
      "\n",
      "  parental_leave months_since_last_training target_variable  \\\n",
      "0              0                          1               0   \n",
      "1              0                          2               0   \n",
      "2              0                          3               0   \n",
      "3              0                          4               0   \n",
      "4              0                          5               0   \n",
      "\n",
      "   ever_terminated_flag  termination_date  \n",
      "0                     0               NaN  \n",
      "1                     0               NaN  \n",
      "2                     0               NaN  \n",
      "3                     0               NaN  \n",
      "4                     0               NaN  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18232 entries, 0 to 18231\n",
      "Data columns (total 43 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   employee_id                 18232 non-null  int64         \n",
      " 1   snapshot_date               18232 non-null  datetime64[ns]\n",
      " 2   age                         18232 non-null  int64         \n",
      " 3   department                  18232 non-null  object        \n",
      " 4   business_unit               18232 non-null  object        \n",
      " 5   job_title                   18232 non-null  object        \n",
      " 6   location                    18232 non-null  object        \n",
      " 7   base_salary                 18232 non-null  int64         \n",
      " 8   bonus_eligible              18232 non-null  int64         \n",
      " 9   bonus_pct                   18232 non-null  float64       \n",
      " 10  equity_grant                18232 non-null  int64         \n",
      " 11  equity_pct                  18232 non-null  float64       \n",
      " 12  employment_type             18232 non-null  object        \n",
      " 13  hire_date                   18232 non-null  object        \n",
      " 14  ethnicity                   18232 non-null  object        \n",
      " 15  marital_status              18232 non-null  object        \n",
      " 16  education_level             18232 non-null  object        \n",
      " 17  pay_frequency               18232 non-null  object        \n",
      " 18  veteran_status              18232 non-null  int64         \n",
      " 19  disability_status           18232 non-null  int64         \n",
      " 20  cost_center                 18232 non-null  object        \n",
      " 21  fte                         18232 non-null  float64       \n",
      " 22  exemption_status            18232 non-null  object        \n",
      " 23  high_potential_flag         18232 non-null  object        \n",
      " 24  succession_plan_status      18232 non-null  object        \n",
      " 25  aihr_certified              18232 non-null  int64         \n",
      " 26  promotion_count             18232 non-null  int64         \n",
      " 27  tenure_months               18232 non-null  int64         \n",
      " 28  performance_rating          18232 non-null  float64       \n",
      " 29  engagement_score            18232 non-null  float64       \n",
      " 30  risk_of_exit_score          18232 non-null  float64       \n",
      " 31  current_salary              18232 non-null  int64         \n",
      " 32  training_count              18232 non-null  int64         \n",
      " 33  job_level                   18232 non-null  int64         \n",
      " 34  last_training_date          11250 non-null  object        \n",
      " 35  vacation_leave              18232 non-null  int64         \n",
      " 36  sick_leave                  18232 non-null  int64         \n",
      " 37  personal_leave              18232 non-null  int64         \n",
      " 38  parental_leave              18232 non-null  int64         \n",
      " 39  months_since_last_training  18232 non-null  int64         \n",
      " 40  target_variable             18232 non-null  int64         \n",
      " 41  ever_terminated_flag        18232 non-null  int64         \n",
      " 42  termination_date            3811 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(6), int64(20), object(16)\n",
      "memory usage: 6.0+ MB\n",
      "None\n",
      "        employee_id                  snapshot_date           age  \\\n",
      "count  18232.000000                          18232  18232.000000   \n",
      "mean     298.908403  2023-07-16 08:42:09.003949312     33.573936   \n",
      "min        1.000000            2021-04-30 00:00:00     22.000000   \n",
      "25%      150.000000            2022-08-23 06:00:00     28.000000   \n",
      "50%      288.000000            2023-08-31 00:00:00     34.000000   \n",
      "75%      431.000000            2024-06-30 00:00:00     38.000000   \n",
      "max      793.000000            2025-03-31 00:00:00     56.000000   \n",
      "std      183.387181                            NaN      6.551226   \n",
      "\n",
      "         base_salary  bonus_eligible     bonus_pct  equity_grant  \\\n",
      "count   18232.000000    18232.000000  18232.000000  18232.000000   \n",
      "mean    93018.777150        0.346698      0.043387      0.813844   \n",
      "min     51548.000000        0.000000      0.000000      0.000000   \n",
      "25%     76769.000000        0.000000      0.000000      1.000000   \n",
      "50%     89957.000000        0.000000      0.000000      1.000000   \n",
      "75%    109326.000000        1.000000      0.096000      1.000000   \n",
      "max    148373.000000        1.000000      0.200000      1.000000   \n",
      "std     20287.686561        0.475932      0.064731      0.389243   \n",
      "\n",
      "         equity_pct  veteran_status  disability_status  ...  current_salary  \\\n",
      "count  18232.000000    18232.000000       18232.000000  ...    18232.000000   \n",
      "mean       0.044539        0.012999           0.063295  ...    98076.643868   \n",
      "min        0.000000        0.000000           0.000000  ...    51548.000000   \n",
      "25%        0.016100        0.000000           0.000000  ...    80918.000000   \n",
      "50%        0.044300        0.000000           0.000000  ...    94458.000000   \n",
      "75%        0.070500        0.000000           0.000000  ...   115426.000000   \n",
      "max        0.100000        1.000000           1.000000  ...   182177.000000   \n",
      "std        0.031648        0.113273           0.243500  ...    22113.566789   \n",
      "\n",
      "       training_count     job_level  vacation_leave    sick_leave  \\\n",
      "count    18232.000000  18232.000000    18232.000000  18232.000000   \n",
      "mean         0.962100      2.108052        0.226909      0.237111   \n",
      "min          0.000000      1.000000        0.000000      0.000000   \n",
      "25%          0.000000      1.000000        0.000000      0.000000   \n",
      "50%          1.000000      2.000000        0.000000      0.000000   \n",
      "75%          1.000000      3.000000        0.000000      0.000000   \n",
      "max          5.000000      5.000000        3.000000      5.000000   \n",
      "std          0.997248      1.040224        0.502000      0.509251   \n",
      "\n",
      "       personal_leave  parental_leave  months_since_last_training  \\\n",
      "count    18232.000000    18232.000000                18232.000000   \n",
      "mean         0.207767        0.207602                38305.634599   \n",
      "min          0.000000        0.000000                    0.000000   \n",
      "25%          0.000000        0.000000                   10.000000   \n",
      "50%          0.000000        0.000000                   32.000000   \n",
      "75%          0.000000        0.000000                99999.000000   \n",
      "max          3.000000        3.000000                99999.000000   \n",
      "std          0.456126        0.479360                48603.108586   \n",
      "\n",
      "       target_variable  ever_terminated_flag  \n",
      "count     18232.000000          18232.000000  \n",
      "mean          0.054739              0.209028  \n",
      "min           0.000000              0.000000  \n",
      "25%           0.000000              0.000000  \n",
      "50%           0.000000              0.000000  \n",
      "75%           0.000000              0.000000  \n",
      "max           1.000000              1.000000  \n",
      "std           0.227476              0.406626  \n",
      "\n",
      "[8 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv('full summary.csv')\n",
    "full_data['snapshot_date'] = pd.to_datetime(full_data['snapshot_date'], format='%d/%m/%Y')\n",
    "print(full_data.head())\n",
    "print(full_data.info())\n",
    "print(full_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba26e6",
   "metadata": {},
   "source": [
    "### 1.3 Initial Data Checks & Preparation\n",
    "\n",
    "Basic checks are performed to understand the dataset's dimensions and the distribution of the target variable (`target_variable`). The data is then sorted by `employee_id` and snapshot_date (descending) to facilitate operations that require the latest employee records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26eb24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (18232, 43)\n",
      "Target variable distribution:\n",
      "target_variable\n",
      "0    17234\n",
      "1      998\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data sorted by employee_id and snapshot_date.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {full_data.shape}\")\n",
    "# Using 'target_variable' based on your df.info() output for consistency\n",
    "print(f\"Target variable distribution:\\n{full_data['target_variable'].value_counts()}\")\n",
    "\n",
    "# Sort data by employee and date - crucial for later filtering of latest snapshots\n",
    "full_data_sorted = full_data.sort_values(by=['employee_id', 'snapshot_date'], ascending=[True, False])\n",
    "print(\"\\nData sorted by employee_id and snapshot_date.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674b2f6",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering & Preprocessing\n",
    "\n",
    "This section focuses on transforming and preparing the data for machine learning, including creating a key feature and handling missing values.\n",
    "\n",
    "### 2.1 Feature Engineering: Employee Tenure\n",
    "`tenure_months` is calculated to represent the duration an employee has been with the company at each snapshot date. This is a crucial feature for attrition prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96003a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id snapshot_date  hire_date  tenure_months\n",
      "0            1    2021-04-30 2020-10-30              5\n",
      "1            1    2021-05-31 2020-10-30              6\n",
      "2            1    2021-06-30 2020-10-30              7\n",
      "3            1    2021-07-31 2020-10-30              9\n",
      "4            1    2021-08-31 2020-10-30             10\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'hire_date' is also a datetime object\n",
    "full_data['hire_date'] = pd.to_datetime(full_data['hire_date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# --- CORRECTED TENURE_MONTHS CALCULATION ---\n",
    "# Calculate the difference in days between snapshot and hire date\n",
    "time_difference_days = (full_data['snapshot_date'] - full_data['hire_date']).dt.days\n",
    "\n",
    "# Convert days to months using an average number of days in a month (~30.44 days/month)\n",
    "full_data['tenure_months'] = (time_difference_days / 30.44).astype(int)\n",
    "\n",
    "print(full_data[['employee_id', 'snapshot_date', 'hire_date', 'tenure_months']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fb038",
   "metadata": {},
   "source": [
    "### 2.2 Handling Missing Values\n",
    "Missing values in key numerical features (`performance_rating`, `engagement_score`) are imputed with their median values to maintain data completeness and prevent errors during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c708dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing 'performance_rating' and 'engagement_score' with their medians\n",
    "for col in ['performance_rating', 'engagement_score']:\n",
    "    if col in full_data.columns and full_data[col].isnull().any():\n",
    "        median_val = full_data[col].median()\n",
    "        full_data[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with median: {median_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a1b74",
   "metadata": {},
   "source": [
    "### 2.3 Defining Features and Target Variable\n",
    "The feature set (`X`) is created by dropping the target variable and identifier columns. The target variable (`y`) is explicitly set to `target_variable`. Numerical and categorical features are then separated for distinct preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b966e268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Features selected for preprocessing: ['age', 'base_salary', 'bonus_eligible', 'bonus_pct', 'equity_grant', 'equity_pct', 'veteran_status', 'disability_status', 'fte', 'aihr_certified', 'promotion_count', 'tenure_months', 'performance_rating', 'engagement_score', 'current_salary', 'training_count', 'job_level', 'vacation_leave', 'sick_leave', 'personal_leave', 'parental_leave', 'months_since_last_training', 'ever_terminated_flag']\n",
      "Categorical Features selected for preprocessing: ['department', 'business_unit', 'job_title', 'location', 'employment_type', 'ethnicity', 'marital_status', 'education_level', 'pay_frequency', 'cost_center', 'exemption_status', 'high_potential_flag', 'succession_plan_status', 'last_training_date']\n"
     ]
    }
   ],
   "source": [
    "# Using 'target_variable' consistently as per your df.info()\n",
    "X = full_data.drop(['target_variable', 'employee_id', 'risk_of_exit_score', 'termination_date', 'snapshot_date', 'hire_date'], axis=1)\n",
    "y = full_data['target_variable'] # Assign 'target_variable' to y\n",
    "\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(f\"Numerical Features selected for preprocessing: {numerical_features}\")\n",
    "print(f\"Categorical Features selected for preprocessing: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8107f5",
   "metadata": {},
   "source": [
    "### 2.4 Data Splitting: Train and Test Sets\n",
    "The dataset is divided into training (80%) and testing (20%) sets. Stratified sampling is used to ensure the proportion of the target variable is maintained in both sets, which is essential for imbalanced datasets. `random_state` ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1013ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14585, 37), y_train shape: (14585,)\n",
      "X_test shape: (3647, 37), y_test shape: (3647,)\n",
      "Train target distribution:\n",
      "target_variable\n",
      "0    0.945286\n",
      "1    0.054714\n",
      "Name: proportion, dtype: float64\n",
      "Test target distribution:\n",
      "target_variable\n",
      "0    0.94516\n",
      "1    0.05484\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(f\"Train target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test target distribution:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3947f35",
   "metadata": {},
   "source": [
    "### 2.5 Preprocessing Pipelines with ColumnTransformer\n",
    "A `ColumnTransformer` is used within a `Pipeline` to apply different preprocessing steps to numerical and categorical features:\n",
    "\n",
    "- **Numerical Features:** Scaled using `StandardScaler` to standardize their range.\n",
    "\n",
    "- **Categorical Features:** Converted to numerical format using `OneHotEncoder`. This approach ensures consistency and prevents data leakage from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9252305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG (2.5 Start): Columns in X_train before preprocessing: ['age', 'department', 'business_unit', 'job_title', 'location', 'base_salary', 'bonus_eligible', 'bonus_pct', 'equity_grant', 'equity_pct', 'employment_type', 'ethnicity', 'marital_status', 'education_level', 'pay_frequency', 'veteran_status', 'disability_status', 'cost_center', 'fte', 'exemption_status', 'high_potential_flag', 'succession_plan_status', 'aihr_certified', 'promotion_count', 'tenure_months', 'performance_rating', 'engagement_score', 'current_salary', 'training_count', 'job_level', 'last_training_date', 'vacation_leave', 'sick_leave', 'personal_leave', 'parental_leave', 'months_since_last_training', 'ever_terminated_flag']\n",
      "DEBUG (2.5 Start): Identified numerical_features: ['age', 'base_salary', 'bonus_eligible', 'bonus_pct', 'equity_grant', 'equity_pct', 'veteran_status', 'disability_status', 'fte', 'aihr_certified', 'promotion_count', 'tenure_months', 'performance_rating', 'engagement_score', 'current_salary', 'training_count', 'job_level', 'vacation_leave', 'sick_leave', 'personal_leave', 'parental_leave', 'months_since_last_training', 'ever_terminated_flag']\n",
      "DEBUG (2.5 Start): Identified categorical_features: ['department', 'business_unit', 'job_title', 'location', 'employment_type', 'ethnicity', 'marital_status', 'education_level', 'pay_frequency', 'cost_center', 'exemption_status', 'high_potential_flag', 'succession_plan_status', 'last_training_date']\n",
      "DEBUG (X_train_processed dtype): float64\n",
      "DEBUG (X_test_processed dtype): float64\n",
      "DEBUG (Post-Transform): Shape of X_train_processed (array): (14585, 227)\n",
      "DEBUG (Post-Transform): Shape of X_test_processed (array): (3647, 227)\n",
      "DEBUG (Feature Names): Length of all_feature_names: 227\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (14585, 1), indices imply (14585, 227)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG (Feature Names): Length of all_feature_names: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_feature_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Convert processed arrays back to DataFrame for easier handling and SHAP compatibility\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# The ValueError occurs here if X_train_processed.shape[1] != len(all_feature_names)\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m X_train_processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_feature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m X_test_processed_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test_processed, columns\u001b[38;5;241m=\u001b[39mall_feature_names, index\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed X_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_processed_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amiru\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    859\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m             arrays,\n\u001b[0;32m    861\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    877\u001b[0m         {},\n\u001b[0;32m    878\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    882\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\amiru\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\amiru\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (14585, 1), indices imply (14585, 227)"
     ]
    }
   ],
   "source": [
    "# --- REVISED CODE START FOR SECTION 2.5 (Paste this entire block) ---\n",
    "\n",
    "# Section 2.5: Preprocessing Pipelines with ColumnTransformer\n",
    "\n",
    "# Ensure these imports are available (typically done at the very top of the notebook)\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import pandas as pd\n",
    "# import numpy as np # For np.number\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Diagnostic: Print the columns in X_train before preprocessing\n",
    "print(f\"\\nDEBUG (2.5 Start): Columns in X_train before preprocessing: {X_train.columns.tolist()}\")\n",
    "print(f\"DEBUG (2.5 Start): Identified numerical_features: {numerical_features}\")\n",
    "print(f\"DEBUG (2.5 Start): Identified categorical_features: {categorical_features}\")\n",
    "\n",
    "# Construct the list of transformers.\n",
    "# Only include transformers if their corresponding feature lists are not empty.\n",
    "transformers_list = []\n",
    "if numerical_features:\n",
    "    transformers_list.append(('num', numeric_transformer, numerical_features))\n",
    "if categorical_features:\n",
    "    transformers_list.append(('cat', categorical_transformer, categorical_features))\n",
    "\n",
    "# If for some reason both lists are empty, ColumnTransformer might rely solely on 'remainder'.\n",
    "if not transformers_list:\n",
    "    print(\"WARNING: No numerical or categorical features found for specific transformations. ColumnTransformer will rely solely on 'remainder'.\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers_list,\n",
    "    remainder='passthrough' # Passes through any columns not explicitly handled\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "print(f\"DEBUG (X_train_processed dtype): {X_train_processed.dtype}\")\n",
    "\n",
    "\n",
    "# Transform the test data using the *fitted* preprocessor\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"DEBUG (X_test_processed dtype): {X_test_processed.dtype}\")\n",
    "\n",
    "# --- DEBUGGING AID: Print processed array shapes immediately after transformation ---\n",
    "# IMPORTANT: This is the actual shape of the numpy array right after the transformation.\n",
    "print(f\"DEBUG (Post-Transform): Shape of X_train_processed (array): {X_train_processed.shape}\")\n",
    "print(f\"DEBUG (Post-Transform): Shape of X_test_processed (array): {X_test_processed.shape}\")\n",
    "# --- END DEBUGGING AID ---\n",
    "\n",
    "# --- CRITICAL CORRECTION: Use preprocessor.get_feature_names_out() to get all output feature names ---\n",
    "# This is the most robust way to get all feature names after ColumnTransformer,\n",
    "# including those passed through by 'remainder', ensuring a perfect match with the output shape.\n",
    "all_feature_names = preprocessor.get_feature_names_out()\n",
    "# --- END OF CRITICAL CORRECTION ---\n",
    "\n",
    "# Diagnostic: Print the length of feature names list\n",
    "print(f\"DEBUG (Feature Names): Length of all_feature_names: {len(all_feature_names)}\")\n",
    "\n",
    "# Convert processed arrays back to DataFrame for easier handling and SHAP compatibility\n",
    "# The ValueError occurs here if X_train_processed.shape[1] != len(all_feature_names)\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=all_feature_names, index=X_train.index)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=all_feature_names, index=X_test.index)\n",
    "\n",
    "print(f\"Processed X_train shape: {X_train_processed_df.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed_df.shape}\")\n",
    "\n",
    "# --- REVISED CODE END FOR SECTION 2.5 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d07593",
   "metadata": {},
   "source": [
    "### 2.6 Addressing Class Imbalance with SMOTE\n",
    "Attrition datasets are typically highly imbalanced (many more employees stay than leave). To prevent models from being biased towards the majority class, we apply `SMOTE` (Synthetic Minority Over-sampling Technique) to the training data. `SMOTE` generates synthetic samples of the minority class (`future_terminated_flag` == 1), balancing the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d427a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing SMOTE code\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed_df, y_train)\n",
    "\n",
    "print(f\"Original training target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nResampled training target distribution:\\n{y_train_resampled.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b03b7",
   "metadata": {},
   "source": [
    "## 3. Model Training & Evaluation\n",
    "This section trains three different machine learning models and evaluates their performance using various classification metrics.\n",
    "\n",
    "### 3.1 Model Selection Rationale\n",
    "For this HR attrition prediction project, a strategic choice of machine learning models was made, prioritizing a balance of interpretability, robustness, and high predictive performance typical for tabular binary classification problems.\n",
    "\n",
    "- Logistic Regression (Interpretable Baseline): Chosen as the foundational linear model. Its interpretability, allowing for direct understanding of feature impacts on attrition likelihood, is invaluable for HR stakeholders. It also provides a strong, efficient baseline for comparison against more complex models.\n",
    "- Random Forest (Robust Ensemble): This ensemble method was selected for its ability to capture complex, non-linear relationships and interactions between features without extensive manual engineering. Its ensemble nature significantly reduces the risk of overfitting, making it a robust choice, and it naturally provides insights into feature importance.\n",
    "- XGBoost (High-Performance Powerhouse): As a state-of-the-art gradient boosting framework, XGBoost was included for its exceptional predictive performance and efficiency. It excels at handling large datasets and complex relationships, often delivering industry-leading accuracy in tabular data competitions. Its built-in regularization also helps prevent overfitting.\n",
    "\n",
    "**Why Other Common Models Were Not Prioritised:**\n",
    "\n",
    "While many other machine learning algorithms exist, several were not the primary focus for this project due to specific trade-offs relative to the problem's requirements:\n",
    "\n",
    "- **Individual Decision Trees:** Prone to severe overfitting, which is effectively mitigated by ensemble methods like Random Forest and XGBoost.\n",
    "Support Vector Machines (SVMs): Can be computationally expensive on larger datasets and sensitive to feature scaling. Interpretability, especially with non-linear kernels, is also a significant challenge for business understanding.\n",
    "\n",
    "- **K-Nearest Neighbors (KNN):** Computationally intensive for prediction on substantial datasets, sensitive to feature scaling, and can suffer from the \"curse of dimensionality\" in higher-dimensional feature spaces.\n",
    "\n",
    "- **Naive Bayes:** Relies on a strong assumption of feature independence, which is rarely true in correlated HR datasets, often leading to suboptimal performance compared to tree-based models.\n",
    "\n",
    "- **Deep Learning / Neural Networks:** While powerful, they are typically overkill and less interpretable for structured tabular data of this nature compared to tree-based models, often requiring more data and computational resources to achieve comparable results. The priority for HR insights leans towards interpretability, making simpler yet powerful models more suitable.\n",
    "\n",
    "### 3.2 Logistic Regression\n",
    "The Logistic Regression model is trained on the resampled training data. Its performance is evaluated using accuracy, precision, recall, F1-score, and ROC AUC, along with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing Logistic Regression training and evaluation code\n",
    "log_reg_model = LogisticRegression(random_state=42, solver='liblinear') # Added solver for robustness\n",
    "log_reg_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_log_reg = log_reg_model.predict(X_test_processed_df)\n",
    "y_prob_log_reg = log_reg_model.predict_proba(X_test_processed_df)[:, 1]\n",
    "\n",
    "print(f\"\\n--- Logistic Regression Performance ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_log_reg):.4f}\")\n",
    "\n",
    "cm_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "disp_log_reg = ConfusionMatrixDisplay(confusion_matrix=cm_log_reg, display_labels=['Stay', 'Terminated'])\n",
    "disp_log_reg.plot()\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57e7ec",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest Classifier\n",
    "A Random Forest model is trained, leveraging its ensemble nature to achieve robust predictions. Its performance is evaluated using the same set of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing Random Forest training and evaluation code\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_rf = rf_model.predict(X_test_processed_df)\n",
    "y_prob_rf = rf_model.predict_proba(X_test_processed_df)[:, 1]\n",
    "\n",
    "print(f\"\\n--- Random Forest Performance ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_rf):.4f}\")\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Stay', 'Terminated'])\n",
    "disp_rf.plot()\n",
    "plt.title('Confusion Matrix for Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef640b",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost Classifier\n",
    "XGBoost, a highly efficient gradient boosting algorithm, is trained to leverage its powerful predictive capabilities. Its performance is critically assessed, as it often provides the best balance of precision and recall for imbalanced classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d940680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing XGBoost training and evaluation code\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss') # Added eval_metric for modern XGBoost\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_xgb = xgb_model.predict(X_test_processed_df)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test_processed_df)[:, 1]\n",
    "\n",
    "print(f\"\\n--- XGBoost Performance ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_xgb):.4f}\")\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=['Stay', 'Terminated'])\n",
    "disp_xgb.plot()\n",
    "plt.title('Confusion Matrix for XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16426541",
   "metadata": {},
   "source": [
    "## 4. Model Interpretability with SHAP\n",
    "Understanding why a model makes certain predictions is crucial for actionable insights. SHAP (SHapley Additive exPlanations) values are used to explain the output of the XGBoost model by showing the contribution of each feature to the prediction.\n",
    "\n",
    "### 4.1 SHAP Summary Plot\n",
    "The SHAP summary plot provides a global view of feature importance, indicating which features have the largest impact on the model's output across the entire dataset, and whether their impact is positive (increasing attrition probability) or negative (decreasing attrition probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing SHAP code\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test_processed_df)\n",
    "if isinstance(shap_values, list): # For multi-output models\n",
    "    shap_values = shap_values[1] # Use SHAP values for the positive class\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_processed_df, feature_names=all_feature_names, show=False)\n",
    "plt.title('SHAP Summary Plot for XGBoost Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4ea38",
   "metadata": {},
   "source": [
    "## 5. Exporting Predictions for Tableau\n",
    "This final section prepares and exports two key datasets for visualization and further analysis in Tableau: `main_for_tableau.csv` (for historical context and model comparison) and `current_snapshot.csv` (for identifying currently at-risk employees).\n",
    "\n",
    "### 5.1 Generate Predictions for Full Dataset\n",
    "We apply all trained models (Logistic Regression, Random Forest, XGBoost) to the entire preprocessed dataset to generate predictions and probabilities for every employee snapshot. This comprehensive dataset will be used for historical analysis and dashboarding in Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code for generating predictions on the full dataset\n",
    "# Ensure X_full_transformed_df is generated correctly based on the full_data\n",
    "# Using the preprocessor on the full_data to transform it consistently\n",
    "\n",
    "X_full_processed = preprocessor.transform(X) # Assuming X is the full feature set from earlier\n",
    "X_full_processed_df = pd.DataFrame(X_full_processed, columns=all_feature_names, index=X.index)\n",
    "\n",
    "\n",
    "full_data_with_preds = full_data.copy()\n",
    "\n",
    "full_data_with_preds['logreg_prediction'] = log_reg_model.predict(X_full_processed_df)\n",
    "full_data_with_preds['logreg_probability'] = log_reg_model.predict_proba(X_full_processed_df)[:, 1]\n",
    "\n",
    "full_data_with_preds['random_forest_prediction'] = rf_model.predict(X_full_processed_df)\n",
    "full_data_with_preds['random_forest_probability'] = rf_model.predict_proba(X_full_processed_df)[:, 1]\n",
    "\n",
    "full_data_with_preds['xgboost_prediction'] = xgb_model.predict(X_full_processed_df)\n",
    "full_data_with_preds['xgboost_probability'] = xgb_model.predict_proba(X_full_processed_df)[:, 1]\n",
    "\n",
    "# Save the primary dataset for Tableau\n",
    "full_data_with_preds.to_csv('main_for_tableau.csv', index=False)\n",
    "print(\"Exported 'main_for_tableau.csv' with all historical snapshots and model predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fdfc9",
   "metadata": {},
   "source": [
    "### 5.2 Filter & Export Current Snapshot for Active Employees\n",
    "To provide actionable insights for immediate intervention, we filter the dataset to create `current_snapshot.csv`. This file contains only the latest snapshot for each active employee (those not already terminated) whose last snapshot date is recent (e.g., from 2025-01-01 onwards). This ensures that HR focuses on current, relevant attrition risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code for filtering current_snapshot.csv\n",
    "latest_per_employee_data_all = full_data_with_preds.sort_values(by=['employee_id', 'snapshot_date'], ascending=[True, False])\\\n",
    "                                                    .drop_duplicates(subset=['employee_id'], keep='first').copy()\n",
    "\n",
    "recent_snapshot_threshold = pd.to_datetime('2025-01-01') # Adjust this date as needed\n",
    "current_snapshot_df = latest_per_employee_data_all[\n",
    "    (latest_per_employee_data_all['ever_terminated_flag'] == 0) & # Filter for active employees\n",
    "    (latest_per_employee_data_all['snapshot_date'] >= recent_snapshot_threshold) # Filter for recent snapshots\n",
    "].copy()\n",
    "\n",
    "current_snapshot_df.to_csv('current_snapshot.csv', index=False)\n",
    "print(f\"Exported 'current_snapshot.csv' with the final snapshot for truly current, active employees.\")\n",
    "print(f\"Number of employees in current_snapshot.csv after recency filter: {len(current_snapshot_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931db19f",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "With the `main_for_tableau.csv` and `current_snapshot.csv` files generated, the next phase involves building interactive dashboards in Tableau to visualize these insights for HR stakeholders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
