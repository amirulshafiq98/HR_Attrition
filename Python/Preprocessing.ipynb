{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a50211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing.ipynb\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ----- CONTROL FLAG -----\n",
    "overwrite_files = True  # <<< Set this to TRUE because we MUST re-process the data\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('full summary.csv')\n",
    "\n",
    "# Convert dates\n",
    "df['snapshot_date'] = pd.to_datetime(df['snapshot_date'])\n",
    "df['hire_date'] = pd.to_datetime(df['hire_date'])\n",
    "df['termination_date'] = pd.to_datetime(df['termination_date'], errors='coerce')\n",
    "\n",
    "# Step 1: Sort data to correctly identify future events per employee\n",
    "df = df.sort_values(by=['employee_id', 'snapshot_date']).reset_index(drop=True)\n",
    "\n",
    "# Define the prediction horizon (e.g., 30 days)\n",
    "PREDICTION_HORIZON_DAYS = 90 # HR usually wants to know about departures in the next month or quarter\n",
    "\n",
    "# Initialize a new target column for future attrition\n",
    "df['future_terminated_flag'] = 0\n",
    "\n",
    "# Loop through employees to determine if they terminated within the horizon\n",
    "for employee_id in df['employee_id'].unique():\n",
    "    employee_data = df[df['employee_id'] == employee_id].copy()\n",
    "    \n",
    "    # Get the actual termination date for this employee from their records\n",
    "    # If an employee terminated, their `terminated_flag` would be 1 and `termination_date` would be populated\n",
    "    actual_termination_date = employee_data['termination_date'].dropna().min() # Get their *first* termination date if multiple records\n",
    "    \n",
    "    if pd.notna(actual_termination_date):\n",
    "        # For each snapshot of this employee *before* their termination, check if they left within the horizon\n",
    "        active_snapshots_before_term = employee_data[employee_data['snapshot_date'] < actual_termination_date].index\n",
    "        \n",
    "        for idx in active_snapshots_before_term:\n",
    "            snapshot_date = df.loc[idx, 'snapshot_date']\n",
    "            \n",
    "            # If the actual termination date is within the prediction horizon of this snapshot date\n",
    "            if (actual_termination_date - snapshot_date).days <= PREDICTION_HORIZON_DAYS:\n",
    "                df.loc[idx, 'future_terminated_flag'] = 1\n",
    "    # Handle cases where employee never terminated in the dataset (future_terminated_flag remains 0)\n",
    "\n",
    "print(f\"Number of future terminations identified (1s): {df['future_terminated_flag'].sum()}\")\n",
    "print(f\"Original ever_terminated_flag (at snapshot) still present for comparison: {df['ever_terminated_flag'].sum()}\")\n",
    "\n",
    "\n",
    "# --- Feature engineering (add new features related to dates) ---\n",
    "df['months_since_hire'] = (df['snapshot_date'] - df['hire_date']).dt.days // 30\n",
    "# Add a feature for 'months_since_last_training' if it's not already well-captured\n",
    "\n",
    "# Define columns to exclude from the model's features (X), but keep in the original dataframe for later export.\n",
    "model_features_to_exclude = ['employee_id', 'snapshot_date', 'hire_date', 'termination_date', 'ever_terminated_flag', 'risk_of_exit_score', 'target_variable']\n",
    "\n",
    "# Define target and features for the model\n",
    "X = df.drop(columns=['future_terminated_flag'] + model_features_to_exclude)\n",
    "y = df['future_terminated_flag']\n",
    "\n",
    "# For documentation and audit purposes (adjust dropped_features.txt content if needed)\n",
    "safe_features = X.columns.tolist() # These are the features going into the model\n",
    "leakage_or_id_features = model_features_to_exclude # These are the columns excluded from the model's X\n",
    "\n",
    "with open('safe_features.txt', 'w') as f:\n",
    "    for feature in safe_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "with open('dropped_features.txt', 'w') as f:\n",
    "    for feature in leakage_or_id_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(\"Safe features and dropped features documented.\")     # <<< Use new target here\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Split data into training and testing sets BEFORE any preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Preprocessing pipelines (rest of this section is largely the same)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform both the training and testing data\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# Instantiate SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE ONLY to the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "# --- SAVE ARTIFACTS ---\n",
    "if overwrite_files:\n",
    "    print(\"Saving new preprocessed files...\")\n",
    "    \n",
    "    # Save preprocessor\n",
    "    with open('preprocessor.pkl', 'wb') as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "\n",
    "    # Get feature names from the fitted preprocessor\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "    # Save the resampled training data\n",
    "    pd.DataFrame(X_train_resampled, columns=feature_names).to_csv('X_train_resampled.csv', index=False)\n",
    "    y_train_resampled.to_csv('y_train_resampled.csv', index=False)\n",
    "\n",
    "    # Save the transformed testing data\n",
    "    pd.DataFrame(X_test_transformed, columns=feature_names).to_csv('X_test_transformed.csv', index=False)\n",
    "    y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "    # Save the FULL original test set data (all columns from 'df' after feature engineering/target creation)\n",
    "    # This 'df' still contains all original columns + new features like 'months_since_hire' and 'future_terminated_flag'.\n",
    "    original_test_set_full_data = df.loc[X_test.index].copy()\n",
    "    original_test_set_full_data.to_csv('original_test_set_full_data.csv', index=False)\n",
    "    print(\"Full original test set data saved for modeling export.\")\n",
    "\n",
    "    print(\"Preprocessing completed and files saved. NOW RUN MODELING.IPYNB\")\n",
    "else:\n",
    "    print(\"overwrite_files=False --> Skipping file save. Using existing X_train/X_test CSV files.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
